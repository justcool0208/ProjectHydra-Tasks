# Kubernetes-Based Stock Monitoring with Chaos Engineering

**Tasks 3-C & 3-D - Project HYDRA**

## Overview

This project demonstrates the migration of a **Docker Compose–based stock monitoring system** to **Kubernetes**, followed by **observability integration** and **chaos engineering experiments** to validate system resilience, self-healing, and recovery under failure conditions.

The system consists of:

* Backend Stock Price API
* Frontend Web Dashboard
* Prometheus for metrics collection
* Grafana for visualization
* Chaos Mesh for fault injection

---

## Task 3-C: Kubernetes Migration & Observability

### Kubernetes Architecture

All services are deployed inside a dedicated namespace:

```bash
stock-monitoring
```

### Deployed Components

| Component   | Description                                     |
| ----------- | ----------------------------------------------- |
| Backend API | Stock price service exposing Prometheus metrics |
| Frontend    | Web dashboard consuming backend API             |
| Prometheus  | Metrics scraping and storage                    |
| Grafana     | Visualization and dashboards                    |
| Chaos Mesh  | Chaos engineering experiments                   |

### Kubernetes Resources Used

* `Deployment`
* `Service`
* `ConfigMap`
* `Namespace`
* `ServiceMonitor` (Prometheus Operator compatible)

---

## Kubernetes Manifests Structure

```
k8s/
├── backend-deployment.yaml
├── backend-service.yaml
├── backend-servicemonitor.yaml
├── frontend-deployment.yaml
├── frontend-service.yaml
└── frontend-configmap.yaml
```

---

## Observability Setup

### Prometheus Metrics Collected

* Total API requests
* Request latency
* Error rate
* Pod CPU usage
* Pod memory usage
* Pod restart count

Prometheus is configured to scrape:

* Backend application metrics
* Kubernetes pod and node metrics

---

## Grafana Dashboards

Grafana is connected to Prometheus and includes panels for:

* API request rate
* API latency (p95)
* Error rate
* Pod CPU usage
* Pod memory usage
* Pod restart count

> Dashboards are Kubernetes-aware and reflect both application and infrastructure health.

---

## Task 3-D: Chaos Engineering with Chaos Mesh

Chaos Mesh is deployed in a **dedicated namespace** and used to inject controlled failures into the backend service.

### Chaos Experiment Files

```
chaos_tests/
├── kill_one_pod.yaml
├── kill_all_pods.yaml
├── cpu_stress.yaml
├── memory_stress.yaml
└── api_delay_injection.yaml
```

---

## Chaos Experiments & Observations

### 1) Pod Kill (Single & Multiple)

**Chaos Type:** `PodChaos`
**Scenario:** Pod crash, bad deployment

**Observation:**

* Pods are automatically recreated by Kubernetes
* Service remains reachable
* Pod restart count increases
* Temporary error spike observed

Screenshot: *Killed one backend pod*
<img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/206bc7bd-be6c-4c29-acd0-ca0978fd00c6" />
Screenshot: *Killed all backend pods*
<img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/b26db8ec-e4a0-4aa6-8b1d-a3cbc9d7e393" />

---

### 2) CPU Stress (90%)

**Chaos Type:** `StressChaos (cpu)`
**Scenario:** Traffic spike / inefficient logic

**Observation:**

* CPU usage rises to ~90%
* API latency increases
* Frontend continues to function (slower updates)
* No crashes observed

Screenshot: *CPU stress for 120 seconds on one pod*
<img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/5010607b-d92c-4513-8306-13601b8a99f3" />

Observation: *Delayed Frontend update (Stock changed from GOOGL to GOOG)*
<img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/7cea490c-d584-44d5-a8cb-7a39abb8ba10" />


---

### 3) Network Delay Injection

**Chaos Type:** `NetworkChaos (delay)`
**Scenario:** Network congestion / slow external API

**Observation:**

* Increased p95 latency
* Frontend renders data with visible delay
* No request timeouts or crashes

Screenshot: *p95 latency spike*
<img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/f81f4f0c-40a6-4b5e-8ec1-8c4e104a01f2" />

Observation: *Frontend dalayed noticed again for the same*
<img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/54437313-f69f-4b72-be6c-d22c780ab618" />


---

### 4) Memory Pressure

**Chaos Type:** `StressChaos (memory)`
**Scenario:** Memory leak / large responses

**Observation:**

* Pod hits memory limit and gets OOM-killed
* Kubernetes restarts the pod automatically
* Service recovers without manual intervention

Screenshot: *Memory pressure on single pod*
<img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/3fc66b9b-dd27-4d6a-9dbd-0b5ca9723ebe" />

---

### 5) Combined Chaos (CPU Stress + Pod Kill)

**Observation:**

* System continues to serve requests
* Temporary latency and error spikes
* Full recovery after chaos ends

Screenshot: *CPU Stress + Pod Kill*
<img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/faf20b41-6544-4bfe-8cb2-c59bf4e85d3c" />

---

## Self-Healing & Recovery Summary

| Failure Type      | Result                      |
| ----------------- | --------------------------- |
| Pod crash         | Auto-restarted              |
| CPU saturation    | Latency increased, no crash |
| Network delay     | Graceful degradation        |
| Memory exhaustion | Pod restarted               |
| Combined chaos    | System recovered            |

* Kubernetes self-healing verified
* Observability validated using Grafana
* Chaos experiments successfully executed


---

## Repository Structure

```
.
├── chaos_tests/
├── k8s/
├── docker-compose.yml
├── Dockerfile
├── main.py
├── prometheus.yml
├── grafana-safe-values.yaml
├── index.html
├── requirements.txt
└── README.md
```

---

## Conclusion

This Task successfully demonstrates:

* Kubernetes migration
* Kubernetes-native observability
* Real-world chaos engineering
* System resilience and self-recovery

---

